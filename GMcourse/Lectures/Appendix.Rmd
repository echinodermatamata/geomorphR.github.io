---
title: "Appendix"
author: ""
subtitle: "A series of helpful parts that did not fit neatly into lectures."
output:
  xaringan::moon_reader:
    css: ["xaringan-themer.css", "utilities.css"]
    nature:
      highlightStyle: solarized-light
      highlightLines: true
      countIncrementalSlides: true
      ratio: '16:9'
      self_contained: false


---

```{r setup, include=FALSE}
library(knitr)
library(Matrix)
library(RRPP)
library(geomorph)
library(scatterplot3d)
knitr::opts_chunk$set(echo = FALSE)

library(xaringanthemer)
style_mono_light()
```

### Matrix Centering

Matrix centering is the process of subtracting from every column of a matrix its mean.  After, the column means of the .blue[**mean-centered**] matrix will all be 0.  Procrustes coordinates are mean-centered via GPA.

Let $\mathbf{1}$ be an $n \times 1$ vector of 1s, with $n$ the same number of observations as in the $n \times p$ matrix of data, $\mathbf{Y}$.

Let $\mathbf{H}$ be the $n \times n$ idempotent matrix found as $\mathbf{H} = \mathbf{1}(\mathbf{1}^T\mathbf{1})^{-1}\mathbf{1}^T$.

Then, .blue[**mean-centered**] data are found as:

$$\mathbf{Z} = \mathbf{Y} - \mathbf{HY} = (\mathbf{I} - \mathbf{H})
\mathbf{Y}$$
---

### Matrix Standardization

Standardization of data might be useful for continuous data variables that are on different scales.  (.red[This is never a concern for Procrustes coordinates.]).  After standardization, each variable in a matrix will have a mean of 0 and each value will be a standard deviation.  This is the matrix concept of variable standarization; i.e., $z=\frac{y - \hat\mu}{\hat\sigma}$

Let $\mathbf{Z}_c$ be the centered data found as $\mathbf{Z}_c =(\mathbf{I} -\mathbf{H})\mathbf{Y}$, as demonstrated in the slide on **Matrix Centering**.

Let $\hat{\mathbf{\Sigma}}$ be the $n \times n$ residual covariance matrix found as $(n-1)^{-1}\mathbf{Z}_c^T\mathbf{Z}_c$.

Let $\mathbf{S}$ be a diagonal matrix of standard deviations, found from the square-roots of the variances along the diagonal of $\hat{\mathbf{\Sigma}}$.

Then, $\mathbf{Z}_s$ are the standardized data found as 

$$\mathbf{Z}_s = \mathbf{S}^{-1}\mathbf{Z}_c\mathbf{S}^{-1}$$
---

### Generalized Least-Squares Estimation, coefficients

The general formula for estimation of linear model coefficients is:

$$\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{\Omega}^{-1}\mathbf{X})\mathbf{X}^T\mathbf{\Omega}^{-1}\mathbf{Z}$$
These coefficients are *generalized*, as they assume that the non-independence among observations can be explained by the hypothetical covariance matrix, $\mathbf{\Omega}$.

The equation above could be *simplified* if observations could be considered independent, i.e., substituting $\mathbf{I}$ for $\mathbf{\Omega}$.  This is shown in the next slide.
---

### Ordinary Least-Squares Estimation, coefficients

The general formula for estimation of linear model coefficients is:

$$\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{\Omega}^{-1}\mathbf{X})\mathbf{X}^T\mathbf{\Omega}^{-1}\mathbf{Z}$$
In the case that **observations can be considered independent**, then we assume,

$$\mathbf{\Omega} = \mathbf{I}$$
Substituting $\mathbf{I}$ for $\mathbf{\Omega}$, above, yields

$$\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{I}^{-1}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{I}^{-1}\mathbf{Z} =  (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Z}$$

---

### Generalized Least-Squares Estimation, covariance matrix

The general formula for estimation of the residual covariance matrix from a linear model is:

$$\hat{\boldsymbol{\Sigma}} = (n-k)^{-1} \left(\mathbf{Z} - \mathbb{E}\left(\mathbf{Z}\right)\right)^T \mathbf{\Omega}^{-1} \left(\mathbf{Z} - \mathbb{E}\left(\mathbf{Z}\right)\right)$$
Where:
+ $\mathbb{E}(\mathbf{Z})$ indicates the expected values, which are the fitted values, $\hat{\mathbf{Z}} = \mathbf{HZ} = \mathbf{X}(\mathbf{\tilde{X}}^T\mathbf{\Omega}^{-1}\mathbf{\tilde{X}})\mathbf{\tilde{X}}\mathbf{\Omega}^{-1}\mathbf{Z}$.  
+ $\mathbf{\Omega}$ is the $n \times n$ covariance matrix that describes the non-independence among the $n$ observations in the $n \times p$ data matrix, $\mathbf{Z}$.
+ $k$ is the number of parameters (columns) in the linear model design matrix, $\mathbf{X}$.

Note the follwoing algebraic equivalency:

$$\hat{\boldsymbol{\Sigma}} = (n-k)^{-1} \left(\mathbf{Z} - \hat{\mathbf{Z}} \right)^T \mathbf{\Omega}^{-1} \left(\mathbf{Z} - \hat{\mathbf{Z}}\right)$$
$$\hat{\boldsymbol{\Sigma}} = (n-k)^{-1} \left(\mathbf{Z} - \hat{\mathbf{Z}} \right)^T \mathbf{\Omega}^{-1/2} \mathbf{\Omega}^{-1/2}\left(\mathbf{Z} - \hat{\mathbf{Z}}\right)$$
---

### Generalized Least-Squares Estimation, covariance matrix (cont.)

$$\hat{\boldsymbol{\Sigma}} = (n-k)^{-1} \left(\mathbf{\Omega}^{-1/2}\mathbf{Z} - \mathbf{\Omega}^{-1/2}\hat{\mathbf{Z}} \right)^T \left(\mathbf{\Omega}^{-1/2}\mathbf{Z} - \mathbf{\Omega}^{-1/2}\hat{\mathbf{Z}}\right)$$
$$\hat{\boldsymbol{\Sigma}} = (n-k)^{-1} \left(\mathbf{\Omega}^{-1/2} (\mathbf{I} - \mathbf{H})\mathbf{Z} \right)^T \left(\mathbf{\Omega}^{-1/2} (\mathbf{I} - \mathbf{H})\mathbf{Z} \right)$$

$$\hat{\boldsymbol{\Sigma}} = (n-k)^{-1} \left(\mathbf{\Omega}^{-1/2} \mathbf{I} - \mathbf{\tilde{H}}\mathbf{Z} \right)^T \left(\mathbf{\Omega}^{-1/2}\mathbf{I} - \mathbf{\tilde{H}}\mathbf{Z} \right)$$
In other words, the covariance matrix for GLS estimation requires using transformed GLS residuals, not GLS mean-centered residuals.

---

### Ordinary Least-Squares Estimation, covariance matrix

The general formula for estimation of the residual covariance matrix from a linear model is:

$$\hat{\boldsymbol{\Sigma}} = (n-k)^{-1} \left(\mathbf{Z} - \mathbb{E}\left(\mathbf{Z}\right)\right)^T \mathbf{\Omega}^{-1} \left(\mathbf{Z} - \mathbb{E}\left(\mathbf{Z}\right)\right)$$

but because observations are considered independent, this simplifies to

$$\hat{\boldsymbol{\Sigma}} = (n-k)^{-1} \left(\mathbf{Z} - \mathbb{E}\left(\mathbf{Z}\right)\right)^T \mathbf{I}^{-1} \left(\mathbf{Z} - \mathbb{E}\left(\mathbf{Z}\right)\right),$$

which simplifies again to 

$$\hat{\boldsymbol{\Sigma}} = (n-k)^{-1} \left(\mathbf{Z} - \mathbb{E}\left(\mathbf{Z}\right)\right)^T  \left(\mathbf{Z} - \mathbb{E}\left(\mathbf{Z}\right)\right).$$

Where:
+ $\mathbb{E}(\mathbf{Z})$ indicates the expected values, which are the fitted values, $\hat{\mathbf{Z}} = \mathbf{HZ} = \mathbf{X}(\mathbf{X}^T\mathbf{X})\mathbf{X}\mathbf{Z}$.  
+ $k$ is the number of parameters (columns) in the linear model design matrix, $\mathbf{X}$.
+ And because $\mathbb{E} \left(\mathbf{Z}\right) = \hat{\mathbf{Z}}$

$$\hat{\boldsymbol{\Sigma}} = (n-k)^{-1} \left(\mathbf{Z} - \hat{\mathbf{Z}}\right)^T  \left(\mathbf{Z} - \hat{\mathbf{Z}}\right) = (n-k)^{-1} \mathbf{E}^T\mathbf{E} $$
---

### Matrix square-root

A matrix square-root (of a symmetric matrix) is a matrix that fits the condition that:

$$\mathbf{\Omega}^{1/2}(\mathbf{\Omega}^{1/2})^T = \mathbf{\Omega}$$
There are multiple ways to obtain $\mathbf{\Omega}^{1/2}$, and choice of method will influence the properties of the result.

**Diagonalization**

The foremost method is **diagonalization** via SVD.  If we solve,

$$\mathbf{\Omega} = \mathbf{V\Lambda V}^T,$$
then 
$$\mathbf{\Omega}^{1/2} = \mathbf{V \Lambda^{1/2} V}^T,$$

where $\mathbf{\Lambda}^{1/2}$ is a diagonal matrix with values equal to $\lambda_i^{1/2}$.

---

### Matrix square-root (cont.)

A matrix square-root (of a symmetric matrix) is a matrix that fits the condition that:

$$\mathbf{\Omega}^{1/2}(\mathbf{\Omega}^{1/2})^T = \mathbf{\Omega}$$
There are multiple ways to obtain $\mathbf{\Omega}^{1/2}$, and choice of method will influence the properties of the result.

**Cholesky decomposition**

Another common method is Cholesky decomposition, which finds,

$$\mathbf{\Omega} = \mathbf{TT}^T,$$
where $\mathbf{T}$ is a triangular matrix.  Unlike diagonalization, the the resultant matrix is not symmetric, so if a Cholesky decomposition is used, the transpose of the matrix, $\mathbf{T}^T$, has to be considered carefully to avoid computational errors.  (It is easy to use $\mathbf{T}^T$ rather than $\mathbf{T}$.)

